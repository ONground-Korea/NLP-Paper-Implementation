{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# import"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import WikiText2\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FCzgg5%2FbtqEttXkz91%2FLK5RqukCujicrxQ2kRWt0k%2Fimg.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. CBOW Model\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbTF4tu%2FbtqErKyNHfS%2FNUbfNSKkCF2ktHwVleDknK%2Fimg.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# nn.Embedding\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "\n",
    "EMBEDING_DIM = 300\n",
    "EMBEDING_MAX_NORM = 1\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBEDING_DIM,\n",
    "            max_norm=EMBEDING_MAX_NORM,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(EMBEDING_DIM, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Skip-Gram Model\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdPRcWU%2FbtqEt5nV6nt%2FgdkM3YokcxtAQZVq5unMp1%2Fimg.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBEDING_DIM,\n",
    "            max_norm=EMBEDING_MAX_NORM,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(EMBEDING_DIM, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "MIN_WORD_FREQUENCY = 50\n",
    "\n",
    "\n",
    "# build_vocab_from iterator #\n",
    "# iterator – Iterator used to build Vocab. Must yield list or iterator of tokens.\n",
    "# min_freq – The minimum frequency needed to include a token in the vocabulary.\n",
    "# specials – Special symbols to add. The order of supplied tokens will be preserved.\n",
    "# special_first – Indicates whether to insert symbols at the beginning or at the end.\n",
    "# max_tokens – If provided, creates the vocab from the max_tokens - len(specials) most frequent tokens.\n",
    "\n",
    "def build_vocab(data_iter, tokenizer):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        map(tokenizer, data_iter),\n",
    "        specials=['<UNK>'],\n",
    "        min_freq=MIN_WORD_FREQUENCY,\n",
    "    )\n",
    "\n",
    "    vocab.set_default_index(vocab['<UNK>'])\n",
    "    return vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# CBOW Collate_fn\n",
    "\n",
    "CBOW_N_WORDS = 4\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "\n",
    "\n",
    "def collate_CBOW(batch, text_pipeline):\n",
    "    '''\n",
    "    데이터로더에 사용될 collate_fn\n",
    "\n",
    "    context는 N=CBOW_NWORDS개의 이전 단어들과 N개의 이후 단어들로 구성된다.\n",
    "\n",
    "    긴 문장은 MAX_SEQUENCE_LENGTH를 넘지 않도록 잘라낸다.\n",
    "\n",
    "    batch_input은 (CBOW_WORDS * 2개의 context들)로 구성된다.\n",
    "    batch_output은 middle word로 구성된다.\n",
    "\n",
    "    :param batch:\n",
    "    :param text_pipeline:\n",
    "    :return: batch_input, batch_output\n",
    "    '''\n",
    "    batch_input, batch_output = [], []\n",
    "\n",
    "    for text in batch:\n",
    "        text_tokens_ids = text_pipeline(text)\n",
    "\n",
    "        if len(text_tokens_ids) < CBOW_N_WORDS * 2 + 1:\n",
    "            continue\n",
    "\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
    "\n",
    "        for idx in range(len(text_tokens_ids) - CBOW_N_WORDS * 2):\n",
    "            toekn_id_sequence = text_tokens_ids[idx:idx + CBOW_N_WORDS * 2 + 1]\n",
    "            output = toekn_id_sequence.pop(CBOW_N_WORDS)\n",
    "            input_ = toekn_id_sequence\n",
    "\n",
    "            batch_input.append(input_)\n",
    "            batch_output.append(output)\n",
    "\n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "\n",
    "    return batch_input, batch_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Skip-Gram Collate_fn\n",
    "\n",
    "SKIPGRAM_N_WORDS = 4\n",
    "\n",
    "\n",
    "def collate_SkipGram(batch, text_pipeline):\n",
    "    '''\n",
    "    :param batch:\n",
    "    :param text_pipeline:\n",
    "\n",
    "    :return: 'batch_input' : middle word, 'batch_output' : context words\n",
    "    '''\n",
    "\n",
    "    batch_input, batch_output = [], []\n",
    "\n",
    "    for text in batch:\n",
    "        text_token_ids = text_pipeline(text)\n",
    "\n",
    "        if len(text_token_ids) < SKIPGRAM_N_WORDS * 2 + 1:\n",
    "            continue\n",
    "\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            text_token_ids = text_token_ids[:MAX_SEQUENCE_LENGTH]\n",
    "\n",
    "        for idx in range(len(text_token_ids) - SKIPGRAM_N_WORDS * 2):\n",
    "            token_id_sequence = text_token_ids[idx:idx + SKIPGRAM_N_WORDS * 2 + 1]\n",
    "            input_ = token_id_sequence.pop(SKIPGRAM_N_WORDS)\n",
    "            outputs = token_id_sequence\n",
    "\n",
    "            for output in outputs:\n",
    "                batch_input.append(input_)\n",
    "                batch_output.append(output)\n",
    "\n",
    "        batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "        batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "\n",
    "        return batch_input, batch_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_data_iterator(dataset_name, dataset_type, dataset_dir):\n",
    "    if dataset_name == 'WikiText2':\n",
    "        data_iter = WikiText2(root=dataset_dir, split=(dataset_type))\n",
    "\n",
    "    data_iter = to_map_style_dataset(data_iter)\n",
    "\n",
    "    return data_iter\n",
    "\n",
    "\n",
    "def get_dataloader_and_vocab(model_name, dataset_name, dataset_type, dataset_dir, batch_size, shuffle, vocab=None):\n",
    "    data_iter = get_data_iterator(dataset_name, dataset_type, dataset_dir)\n",
    "    tokenizer = get_tokenizer('basic_english', language='en')\n",
    "\n",
    "    if not vocab:\n",
    "        vocab = build_vocab(data_iter, tokenizer)\n",
    "\n",
    "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "    if model_name == 'CBOW':\n",
    "        collate_fn = collate_CBOW\n",
    "    elif model_name == 'SkipGram':\n",
    "        collate_fn = collate_SkipGram\n",
    "    else:\n",
    "        raise ValueError('model_name must be CBOW or SkipGram')\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        data_iter,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=partial(collate_fn, text_pipeline=text_pipeline),\n",
    "    )\n",
    "\n",
    "    return dataloader, vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Train / Validate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_lr_scheduler(optimizer, total_epochs: int, verbose: bool = True):\n",
    "    '''\n",
    "    논문에서는 0.025로 시작해서 각 에포크마다 선형적으로 감소하여 마지막에는 0이 되도록 한다.\n",
    "    '''\n",
    "    lr_lambda = lambda epoch: (total_epochs - epoch) / total_epochs\n",
    "    lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda, verbose=verbose)\n",
    "\n",
    "    return lr_scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_dataloader, vocab = get_dataloader_and_vocab(\n",
    "    model_name='CBOW',\n",
    "    dataset_name='WikiText2',\n",
    "    dataset_type='train',\n",
    "    dataset_dir='data/WikiText2',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    vocab=None,\n",
    ")\n",
    "\n",
    "val_dataloader, _ = get_dataloader_and_vocab(\n",
    "    model_name='CBOW',\n",
    "    dataset_name='WikiText2',\n",
    "    dataset_type='valid',\n",
    "    dataset_dir='data/WikiText2',\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    vocab=vocab\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Vocabulary size: 4100\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "\n",
    "model = CBOW(vocab_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.025)\n",
    "lr_scheduler = get_lr_scheduler(optimizer, epochs, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d368833c5d6d4dba8901a5457d8d970c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, Train Loss=5.01915, Val Loss=5.03971\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Epoch: 2/5, Train Loss=4.95284, Val Loss=5.02065\n",
      "Adjusting learning rate of group 0 to 1.5000e-02.\n",
      "Epoch: 3/5, Train Loss=4.88210, Val Loss=4.94576\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch: 4/5, Train Loss=4.78571, Val Loss=4.88225\n",
      "Adjusting learning rate of group 0 to 5.0000e-03.\n",
      "Epoch: 5/5, Train Loss=4.64527, Val Loss=4.77131\n",
      "Adjusting learning rate of group 0 to 0.0000e+00.\n"
     ]
    }
   ],
   "source": [
    "loss = {'train': [], 'valid': []}\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    ######################## Train #########################\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "\n",
    "    for i, batch_data in enumerate(train_dataloader, 1):\n",
    "        inputs = batch_data[0].to(device)\n",
    "        labels = batch_data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(running_loss)\n",
    "    # loss['train'].append(epoch_loss)\n",
    "\n",
    "    ######################### Validate #############################\n",
    "    model.eval()\n",
    "    running_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(val_dataloader, 1):\n",
    "            inputs = batch_data[0].to(device)\n",
    "            labels = batch_data[1].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(running_loss)\n",
    "    # loss['valid'].append(epoch_loss)\n",
    "\n",
    "    print(\n",
    "        'Epoch: {}/{}, Train Loss={:.5f}, Val Loss={:.5f}'.format(\n",
    "            epoch + 1,\n",
    "            epochs,\n",
    "            train_loss,\n",
    "            val_loss,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    ####################### Save checkpoint ##############################\n",
    "    model_path = 'checkpoint.pt'\n",
    "    torch.save(model, model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Inference"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(4100, 300)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding from first model layer\n",
    "embeddings = list(model.parameters())[0]\n",
    "embeddings = embeddings.cpu().detach().numpy()\n",
    "\n",
    "# normalization\n",
    "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
    "norms = np.reshape(norms, (len(norms), 1))\n",
    "embeddings_norm = embeddings / norms\n",
    "embeddings_norm.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjs\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hjs\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:805: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# get embeddings\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "\n",
    "# t-SNE transform\n",
    "tsne = TSNE(n_components=2)\n",
    "embeddings_df_trans = tsne.fit_transform(embeddings_df)\n",
    "embeddings_df_trans = pd.DataFrame(embeddings_df_trans)\n",
    "\n",
    "# get token order\n",
    "embeddings_df_trans.index = vocab.get_itos()\n",
    "\n",
    "# if token is a number\n",
    "is_numeric = embeddings_df_trans.index.str.isnumeric()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "color = np.where(is_numeric, \"green\", \"black\")\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=embeddings_df_trans[0],\n",
    "        y=embeddings_df_trans[1],\n",
    "        mode=\"text\",\n",
    "        text=embeddings_df_trans.index,\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(color=color),\n",
    "    )\n",
    ")\n",
    "fig.write_html(\"word2vec_visualization.html\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def get_top_similar(word: str, topN: int = 10):\n",
    "    word_id = vocab[word]\n",
    "    if word_id == 0:\n",
    "        print(\"Out of vocabulary word\")\n",
    "        return\n",
    "\n",
    "    word_vec = embeddings_norm[word_id]\n",
    "    word_vec = np.reshape(word_vec, (len(word_vec), 1))\n",
    "    dists = np.matmul(embeddings_norm, word_vec).flatten()\n",
    "    topN_ids = np.argsort(-dists)[1 : topN + 1]\n",
    "\n",
    "    topN_dict = {}\n",
    "    for sim_word_id in topN_ids:\n",
    "        sim_word = vocab.lookup_token(sim_word_id)\n",
    "        topN_dict[sim_word] = dists[sim_word_id]\n",
    "    return topN_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy: 0.497\n",
      "spain: 0.449\n",
      "states: 0.405\n",
      "1962: 0.404\n",
      "australia: 0.400\n",
      "kingdom: 0.398\n",
      "ranging: 0.390\n",
      "1927: 0.385\n",
      "mediterranean: 0.384\n",
      "france: 0.383\n"
     ]
    }
   ],
   "source": [
    "for word, sim in get_top_similar(\"germany\").items():\n",
    "    print(\"{}: {:.3f}\".format(word, sim))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king: 0.735\n",
      "woman: 0.437\n",
      "treaty: 0.402\n",
      "spain: 0.386\n",
      "bones: 0.378\n"
     ]
    }
   ],
   "source": [
    "emb1 = embeddings[vocab[\"king\"]]\n",
    "emb2 = embeddings[vocab[\"man\"]]\n",
    "emb3 = embeddings[vocab[\"woman\"]]\n",
    "\n",
    "emb4 = emb1 - emb2 + emb3\n",
    "emb4_norm = (emb4 ** 2).sum() ** (1 / 2)\n",
    "emb4 = emb4 / emb4_norm\n",
    "\n",
    "emb4 = np.reshape(emb4, (len(emb4), 1))\n",
    "dists = np.matmul(embeddings_norm, emb4).flatten()\n",
    "\n",
    "top5 = np.argsort(-dists)[:5]\n",
    "\n",
    "for word_id in top5:\n",
    "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dists[word_id]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}